{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "08_pytorch_paper_replicating_exercises.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyOhoCjGZZxrecbm76R8UJZn",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ],
   "metadata": {
    "id": "Y5H5P8EjCNGK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4214da9e-f3a8-43e6-a48c-1a33f44a9be9",
    "ExecuteTime": {
     "end_time": "2024-10-15T04:10:05.703433Z",
     "start_time": "2024-10-15T04:10:05.697632Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementatation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T04:10:06.199863Z",
     "start_time": "2024-10-15T04:10:06.109828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.Q = nn.Linear(input_dim, embedding_dim)\n",
    "        self.K = nn.Linear(input_dim, embedding_dim)\n",
    "        self.V = nn.Linear(input_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x_q, x_k, x_v, mask=None):\n",
    "        q = self.Q(x_q)\n",
    "        k = self.K(x_k)\n",
    "        v = self.V(x_v)\n",
    "        \n",
    "        a = (q @ k.permute(0, 2, 1)) / (self.input_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            a[:, mask] = -1e10\n",
    "        x = torch.softmax(a, dim=2) @ v\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention_heads = nn.ModuleList([SelfAttention(input_dim, input_dim // n_heads) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x_q, x_k, x_v, mask=None):\n",
    "        x = torch.concat([attention_head(x_q, x_k, x_v, mask) for attention_head in self.attention_heads], dim=-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "x_batch = torch.randn(4, 100, 512)\n",
    "mask = torch.ones(100, 100).tril() == 0\n",
    "\n",
    "print(MultiheadSelfAttention(512, 16)(x_batch, x_batch, x_batch, mask=mask).shape)\n",
    "print(nn.MultiheadAttention(512, 16, batch_first=True)(x_batch, x_batch, x_batch, need_weights=False, attn_mask=mask)[0].shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 512])\n",
      "torch.Size([4, 100, 512])\n"
     ]
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T04:10:06.325945Z",
     "start_time": "2024-10-15T04:10:06.319679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MSABlock(nn.Module):\n",
    "    def __init__(self, input_dim, n_heads, dropout, torch_msa=True):\n",
    "        super().__init__()\n",
    "        self.torch_msa = torch_msa\n",
    "        \n",
    "        self.msa = MultiheadSelfAttention(input_dim, n_heads) if torch_msa \\\n",
    "            else nn.MultiheadAttention(input_dim, n_heads, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x_q, x_k, x_v, mask=None):\n",
    "        x = self.msa(x_q, x_k, x_v, mask=mask) if self.torch_msa \\\n",
    "            else self.msa(x_q, x_k, x_v, need_weights=False, attn_mask=mask)[0] # mask\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, input_dim, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, mlp_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(mlp_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T04:10:06.471524Z",
     "start_time": "2024-10-15T04:10:06.467348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, mlp_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.msa_block = MSABlock(input_dim, n_heads, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.mlp_block = MLPBlock(input_dim, mlp_dim, dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.msa_block(x, x, x, mask) + x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.mlp_block(x) + x\n",
    "        x = self.layer_norm2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T04:12:39.719699Z",
     "start_time": "2024-10-15T04:12:39.712963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GPT1(nn.Module):\n",
    "    def __init__(self, seq_len, word_dim, embedding_dim, mlp_dim, n_heads, n_layers, n_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        self.input_embedding = nn.Linear(word_dim, embedding_dim, bias=False)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, seq_len, embedding_dim))\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, mlp_dim, n_heads, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.classification_head = nn.Sequential(\n",
    "            nn.LayerNorm(embedding_dim),\n",
    "            nn.Linear(embedding_dim, n_classes)\n",
    "        )\n",
    "    \n",
    "    def create_attention_mask(self):\n",
    "        mask = torch.ones(self.seq_len, self.seq_len).tril() == 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        input_embedding = self.input_embedding(input_seq) + self.positional_encoding\n",
    "        \n",
    "        mask = self.create_attention_mask()\n",
    "        decoder_out = input_embedding\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            decoder_layer(input_embedding, mask)\n",
    "        \n",
    "        logits = self.classification_head(decoder_out)\n",
    "        \n",
    "        return logits"
   ],
   "outputs": [],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T04:12:42.202972Z",
     "start_time": "2024-10-15T04:12:40.058322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seq_len = 512\n",
    "word_dim = 40478\n",
    "embedding_dim = 768\n",
    "mlp_dim = embedding_dim*4\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "dropout = 0.1\n",
    "\n",
    "gpt1_model = GPT1(seq_len, word_dim, embedding_dim, mlp_dim, n_heads, n_layers, word_dim, dropout)\n",
    "summary(gpt1_model, input_size=(1, seq_len, word_dim), device='cpu', col_names=['output_size', 'num_params', 'mult_adds'], depth=2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #                   Mult-Adds\n",
       "==================================================================================================================================\n",
       "GPT1                                                    [1, 512, 40478]           393,216                   --\n",
       "├─Linear: 1-1                                           [1, 512, 768]             31,087,104                31,087,104\n",
       "├─ModuleList: 1-2                                       --                        --                        --\n",
       "│    └─EncoderBlock: 2-1                                [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-2                                [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-3                                [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-4                                [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-5                                [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-6                                [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-7                                [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-8                                [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-9                                [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-10                               [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-11                               [1, 512, 768]             7,087,872                 7,087,872\n",
       "│    └─EncoderBlock: 2-12                               [1, 512, 768]             7,087,872                 7,087,872\n",
       "├─Sequential: 1-3                                       [1, 512, 40478]           --                        --\n",
       "│    └─LayerNorm: 2-13                                  [1, 512, 768]             1,536                     1,536\n",
       "│    └─Linear: 2-14                                     [1, 512, 40478]           31,127,582                31,127,582\n",
       "==================================================================================================================================\n",
       "Total params: 147,663,902\n",
       "Trainable params: 147,663,902\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 147.27\n",
       "==================================================================================================================================\n",
       "Input size (MB): 82.90\n",
       "Forward/backward pass size (MB): 587.33\n",
       "Params size (MB): 589.08\n",
       "Estimated Total Size (MB): 1259.31\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T03:59:12.642607Z",
     "start_time": "2024-10-15T03:59:10.100772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import OpenAIGPTConfig, OpenAIGPTModel\n",
    "config = OpenAIGPTConfig()\n",
    "gpt1_torch_model = OpenAIGPTModel(config)\n",
    "\n",
    "input_tensor = torch.randint(0, word_dim, (1, seq_len), dtype=torch.long) \n",
    "summary(gpt1_torch_model, input_data=input_tensor, device='cpu', col_names=['output_size', 'num_params', 'mult_adds'], depth=2)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #                   Mult-Adds\n",
       "========================================================================================================================\n",
       "OpenAIGPTModel                                [1, 512, 768]             --                        --\n",
       "├─Embedding: 1-1                              [1, 512, 768]             31,087,104                31,087,104\n",
       "├─Embedding: 1-2                              [1, 512, 768]             393,216                   393,216\n",
       "├─Dropout: 1-3                                [1, 512, 768]             --                        --\n",
       "├─ModuleList: 1-4                             --                        --                        --\n",
       "│    └─Block: 2-1                             [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-3                             --                        (recursive)               --\n",
       "│    └─Block: 2-4                             [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-6                             --                        (recursive)               --\n",
       "│    └─Block: 2-7                             [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-9                             --                        (recursive)               --\n",
       "│    └─Block: 2-10                            [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-12                            --                        (recursive)               --\n",
       "│    └─Block: 2-13                            [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-15                            --                        (recursive)               --\n",
       "│    └─Block: 2-16                            [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-18                            --                        (recursive)               --\n",
       "│    └─Block: 2-19                            [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-21                            --                        (recursive)               --\n",
       "│    └─Block: 2-22                            [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-24                            --                        (recursive)               --\n",
       "│    └─Block: 2-25                            [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-27                            --                        (recursive)               --\n",
       "│    └─Block: 2-28                            [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-30                            --                        (recursive)               --\n",
       "│    └─Block: 2-31                            [1, 512, 768]             7,087,872                 11,792,942,592\n",
       "│    └─Block: 2-32                            --                        (recursive)               --\n",
       "│    └─Block: 2-33                            --                        (recursive)               --\n",
       "│    └─Block: 2-34                            [1, 512, 768]             7,087,872                 13,605,473,280\n",
       "========================================================================================================================\n",
       "Total params: 142,512,384\n",
       "Trainable params: 142,512,384\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 163.30\n",
       "========================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 421.53\n",
       "Params size (MB): 466.14\n",
       "Estimated Total Size (MB): 887.67\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 61
  }
 ]
}
