{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "08_pytorch_paper_replicating_exercises.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyOhoCjGZZxrecbm76R8UJZn",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torchinfo import summary\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(DEVICE)"
   ],
   "metadata": {
    "id": "Y5H5P8EjCNGK",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "4214da9e-f3a8-43e6-a48c-1a33f44a9be9",
    "ExecuteTime": {
     "end_time": "2024-10-15T08:59:09.610070Z",
     "start_time": "2024-10-15T08:59:03.942385Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Implementatation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:59:10.206478Z",
     "start_time": "2024-10-15T08:59:10.128541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.Q = nn.Linear(input_dim, embedding_dim)\n",
    "        self.K = nn.Linear(input_dim, embedding_dim)\n",
    "        self.V = nn.Linear(input_dim, embedding_dim)\n",
    "    \n",
    "    def forward(self, x_q, x_k, x_v, mask=None):\n",
    "        q = self.Q(x_q)\n",
    "        k = self.K(x_k)\n",
    "        v = self.V(x_v)\n",
    "        \n",
    "        a = (q @ k.permute(0, 2, 1)) / (self.input_dim ** 0.5)\n",
    "        if mask is not None:\n",
    "            a[:, mask] = -1e10\n",
    "        x = torch.softmax(a, dim=2) @ v\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, n_heads):\n",
    "        super().__init__()\n",
    "        self.attention_heads = nn.ModuleList([SelfAttention(input_dim, input_dim // n_heads) for _ in range(n_heads)])\n",
    "        self.linear = nn.Linear(input_dim, input_dim)\n",
    "    \n",
    "    def forward(self, x_q, x_k, x_v, mask=None):\n",
    "        x = torch.concat([attention_head(x_q, x_k, x_v, mask) for attention_head in self.attention_heads], dim=-1)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "x_batch = torch.randn(4, 100, 512)\n",
    "mask = torch.ones(100, 100).tril() == 0\n",
    "\n",
    "print(MultiheadSelfAttention(512, 16)(x_batch, x_batch, x_batch, mask=mask).shape)\n",
    "print(nn.MultiheadAttention(512, 16, batch_first=True)(x_batch, x_batch, x_batch, need_weights=False, attn_mask=mask)[0].shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 512])\n",
      "torch.Size([4, 100, 512])\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:59:10.248620Z",
     "start_time": "2024-10-15T08:59:10.240198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MSABlock(nn.Module):\n",
    "    def __init__(self, input_dim, n_heads, dropout, torch_msa=True):\n",
    "        super().__init__()\n",
    "        self.torch_msa = torch_msa\n",
    "        \n",
    "        self.msa = MultiheadSelfAttention(input_dim, n_heads) if torch_msa \\\n",
    "            else nn.MultiheadAttention(input_dim, n_heads, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x_q, x_k, x_v, mask=None):\n",
    "        x = self.msa(x_q, x_k, x_v, mask=mask) if self.torch_msa \\\n",
    "            else self.msa(x_q, x_k, x_v, need_weights=False, attn_mask=mask)[0] # mask\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, input_dim, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, mlp_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.linear2 = nn.Linear(mlp_dim, input_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:59:10.265466Z",
     "start_time": "2024-10-15T08:59:10.258621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, mlp_dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.msa_block = MSABlock(input_dim, n_heads, dropout)\n",
    "        self.layer_norm1 = nn.LayerNorm(input_dim)\n",
    "        self.mlp_block = MLPBlock(input_dim, mlp_dim, dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(input_dim)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.msa_block(x, x, x, mask) + x\n",
    "        x = self.layer_norm1(x)\n",
    "        x = self.mlp_block(x) + x\n",
    "        x = self.layer_norm2(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:01:35.535934Z",
     "start_time": "2024-10-15T09:01:35.523766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, seq_len, word_dim, embedding_dim, mlp_dim, n_heads, n_layers, n_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.input_embedding = nn.Linear(word_dim, embedding_dim, bias=False)\n",
    "        self.segment_embedding = nn.Linear(1, embedding_dim)\n",
    "        self.positional_embedding = nn.Linear(1, embedding_dim)\n",
    "        \n",
    "        self.decoder_layers = nn.ModuleList([\n",
    "            DecoderBlock(embedding_dim, mlp_dim, n_heads, dropout) for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.nsp = nn.Linear(embedding_dim, 2)\n",
    "        self.mtp = nn.Linear(embedding_dim, n_classes)\n",
    "    \n",
    "    def forward(self, input_seq, segment):\n",
    "        position = torch.arange(self.seq_len).type(torch.float32).expand(input_seq.shape[0], self.seq_len).unsqueeze(-1)\n",
    "        \n",
    "        segment_embedding = self.segment_embedding(segment)\n",
    "        positional_embedding = self.positional_embedding(position)\n",
    "        input_embedding = self.input_embedding(input_seq) + positional_embedding + segment_embedding\n",
    "        \n",
    "        decoder_out = input_embedding\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            decoder_layer(input_embedding)\n",
    "        \n",
    "        print(decoder_out.shape)\n",
    "        nsp = self.nsp(decoder_out[:, 0])\n",
    "        mtp = self.mtp(decoder_out)\n",
    "\n",
    "        return nsp, mtp"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T09:08:05.008512Z",
     "start_time": "2024-10-15T09:08:02.336818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "seq_len = 512\n",
    "word_dim = 30522\n",
    "embedding_dim = 768\n",
    "mlp_dim = embedding_dim*4\n",
    "n_heads = 12\n",
    "n_layers = 12\n",
    "dropout = 0.1\n",
    "\n",
    "bert_model = BERT(seq_len, word_dim, embedding_dim, mlp_dim, n_heads, n_layers, word_dim, dropout)\n",
    "summary(bert_model, input_size=[(1, seq_len, word_dim), (1, seq_len, 1)], device='cpu', col_names=['output_size', 'num_params', 'mult_adds'], depth=3)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #                   Mult-Adds\n",
       "==================================================================================================================================\n",
       "BERT                                                    [1, 2]                    --                        --\n",
       "├─Linear: 1-1                                           [1, 512, 768]             1,536                     1,536\n",
       "├─Linear: 1-2                                           [1, 512, 768]             1,536                     1,536\n",
       "├─Linear: 1-3                                           [1, 512, 768]             23,440,896                23,440,896\n",
       "├─ModuleList: 1-4                                       --                        --                        --\n",
       "│    └─DecoderBlock: 2-1                                [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-1                               [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-2                              [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-3                               [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-4                              [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-2                                [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-5                               [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-6                              [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-7                               [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-8                              [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-3                                [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-9                               [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-10                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-11                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-12                             [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-4                                [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-13                              [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-14                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-15                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-16                             [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-5                                [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-17                              [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-18                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-19                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-20                             [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-6                                [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-21                              [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-22                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-23                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-24                             [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-7                                [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-25                              [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-26                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-27                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-28                             [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-8                                [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-29                              [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-30                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-31                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-32                             [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-9                                [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-33                              [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-34                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-35                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-36                             [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-10                               [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-37                              [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-38                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-39                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-40                             [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-11                               [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-41                              [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-42                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-43                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-44                             [1, 512, 768]             1,536                     1,536\n",
       "│    └─DecoderBlock: 2-12                               [1, 512, 768]             --                        --\n",
       "│    │    └─MSABlock: 3-45                              [1, 512, 768]             2,362,368                 2,362,368\n",
       "│    │    └─LayerNorm: 3-46                             [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─MLPBlock: 3-47                              [1, 512, 768]             4,722,432                 4,722,432\n",
       "│    │    └─LayerNorm: 3-48                             [1, 512, 768]             1,536                     1,536\n",
       "├─Linear: 1-5                                           [1, 2]                    1,538                     1,538\n",
       "├─Linear: 1-6                                           [1, 512, 30522]           23,471,418                23,471,418\n",
       "==================================================================================================================================\n",
       "Total params: 131,971,388\n",
       "Trainable params: 131,971,388\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 131.97\n",
       "==================================================================================================================================\n",
       "Input size (MB): 62.51\n",
       "Forward/backward pass size (MB): 549.69\n",
       "Params size (MB): 527.89\n",
       "Estimated Total Size (MB): 1140.09\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-15T08:14:12.802180Z",
     "start_time": "2024-10-15T08:14:10.894519Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "config = BertConfig()\n",
    "bert_torch_model = BertModel(config)\n",
    "\n",
    "input_tensor = torch.randint(0, word_dim, (1, seq_len), dtype=torch.long) \n",
    "summary(bert_torch_model, input_data=input_tensor, device='cpu', col_names=['output_size', 'num_params', 'mult_adds'], depth=5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #                   Mult-Adds\n",
       "==================================================================================================================================\n",
       "BertModel                                               [1, 768]                  --                        --\n",
       "├─BertEmbeddings: 1-1                                   [1, 512, 768]             --                        --\n",
       "│    └─Embedding: 2-1                                   [1, 512, 768]             23,440,896                23,440,896\n",
       "│    └─Embedding: 2-2                                   [1, 512, 768]             1,536                     1,536\n",
       "│    └─Embedding: 2-3                                   [1, 512, 768]             393,216                   393,216\n",
       "│    └─LayerNorm: 2-4                                   [1, 512, 768]             1,536                     1,536\n",
       "│    └─Dropout: 2-5                                     [1, 512, 768]             --                        --\n",
       "├─BertEncoder: 1-2                                      [1, 512, 768]             --                        --\n",
       "│    └─ModuleList: 2-6                                  --                        --                        --\n",
       "│    │    └─BertLayer: 3-1                              [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-1                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-1        [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-2               [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-2                  [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-3                       [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-4               [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-3                        [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-5                       [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-6                      [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-7                    [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-2                              [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-4                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-8        [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-9               [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-5                  [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-10                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-11              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-6                        [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-12                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-13                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-14                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-3                              [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-7                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-15       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-16              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-8                  [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-17                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-18              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-9                        [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-19                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-20                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-21                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-4                              [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-10                    [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-22       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-23              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-11                 [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-24                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-25              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-12                       [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-26                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-27                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-28                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-5                              [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-13                    [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-29       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-30              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-14                 [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-31                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-32              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-15                       [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-33                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-34                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-35                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-6                              [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-16                    [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-36       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-37              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-17                 [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-38                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-39              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-18                       [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-40                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-41                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-42                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-7                              [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-19                    [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-43       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-44              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-20                 [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-45                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-46              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-21                       [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-47                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-48                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-49                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-8                              [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-22                    [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-50       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-51              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-23                 [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-52                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-53              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-24                       [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-54                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-55                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-56                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-9                              [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-25                    [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-57       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-58              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-26                 [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-59                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-60              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-27                       [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-61                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-62                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-63                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-10                             [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-28                    [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-64       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-65              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-29                 [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-66                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-67              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-30                       [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-68                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-69                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-70                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-11                             [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-31                    [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-71       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-72              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-32                 [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-73                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-74              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-33                       [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-75                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-76                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-77                   [1, 512, 768]             1,536                     1,536\n",
       "│    │    └─BertLayer: 3-12                             [1, 512, 768]             --                        --\n",
       "│    │    │    └─BertAttention: 4-34                    [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─BertSdpaSelfAttention: 5-78       [1, 512, 768]             1,771,776                 1,771,776\n",
       "│    │    │    │    └─BertSelfOutput: 5-79              [1, 512, 768]             592,128                   592,128\n",
       "│    │    │    └─BertIntermediate: 4-35                 [1, 512, 3072]            --                        --\n",
       "│    │    │    │    └─Linear: 5-80                      [1, 512, 3072]            2,362,368                 2,362,368\n",
       "│    │    │    │    └─GELUActivation: 5-81              [1, 512, 3072]            --                        --\n",
       "│    │    │    └─BertOutput: 4-36                       [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─Linear: 5-82                      [1, 512, 768]             2,360,064                 2,360,064\n",
       "│    │    │    │    └─Dropout: 5-83                     [1, 512, 768]             --                        --\n",
       "│    │    │    │    └─LayerNorm: 5-84                   [1, 512, 768]             1,536                     1,536\n",
       "├─BertPooler: 1-3                                       [1, 768]                  --                        --\n",
       "│    └─Linear: 2-7                                      [1, 768]                  590,592                   590,592\n",
       "│    └─Tanh: 2-8                                        [1, 768]                  --                        --\n",
       "==================================================================================================================================\n",
       "Total params: 109,482,240\n",
       "Trainable params: 109,482,240\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 109.48\n",
       "==================================================================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 427.83\n",
       "Params size (MB): 437.93\n",
       "Estimated Total Size (MB): 865.76\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 68
  }
 ]
}
